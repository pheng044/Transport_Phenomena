{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be67815c",
   "metadata": {},
   "source": [
    "# Linear Least Squares\n",
    "\n",
    "Consider the canonical linear algebra problem,\n",
    "\n",
    "$$\\mathbf{A}\\mathbf{x}=\\mathbf{b},$$ \n",
    "\n",
    "with\n",
    "$\\mathbf{A}\\in\\mathbb{R}^{m\\times n}$, $\\mathbf{x}\\in\\mathbb{R}^{n}$,\n",
    "and $\\mathbf{b}\\in\\mathbb{R}^{m}$. For a general system with $m>n$,\n",
    "there exists no solution. However, we can consider a new problem in\n",
    "which we find the \\\"best\\\" $\\mathbf{x}$ which minimizes the difference,\n",
    "$\\mathbf{A}\\mathbf{x}-\\mathbf{b}$. Geometrically, the $\\mathbf{b}$\n",
    "vector lies in $\\mathbb{R}^{m}$ while the columns of $\\mathbf{A}$ only\n",
    "span at most $\\mathbb{R}^{n}$. If we could project $\\mathbf{b}$ on the\n",
    "column space of $\\mathbf{A}$, then we can find a $\\mathbf{x}$ vector\n",
    "which \\\"best\\\" approximates $\\mathbf{b}$. To start this, consider the\n",
    "standard vector projection of a vector $\\mathbf{u}$ on a vector\n",
    "$\\mathbf{v}$,\n",
    "\n",
    "$$\\ \\mathrm{proj}_{\\mathbf{u}}\\left(\\mathbf{v}\\right)=\\frac{\\mathbf{u}\\cdot\\mathbf{v}}{\\mathbf{v}\\cdot\\mathbf{v}} \\mathbf{v} =\\frac{\\mathbf{u}^{\\mathrm{T}}\\mathbf{v}}{\\mathbf{v}^{\\mathrm{T}}\\mathbf{v}} \\mathbf{v}.$$\n",
    "\n",
    "If we let $\\mathbf{u}$ be a column of $\\mathbf{A}$, $\\mathbf{a}_i$, and\n",
    "$\\mathbf{v}=\\mathbf{b}$, then we are projecting $\\mathbf{b}$ on the\n",
    "allowable range of $\\mathbf{A}$, i.e.,\n",
    "\n",
    "$$\\ \\mathrm{proj}_{\\mathbf{b}}\\left(\\mathbf{a}_i\\right)=\\frac{\\mathbf{a}_i^{\\mathrm{T}}\\mathbf{b}}{\\mathbf{a}_i^{\\mathrm{T}}\\mathbf{a}_i} \\mathbf{a}_i$$\n",
    "\n",
    "$$\\ =\\mathbf{a}_i\\left(\\mathbf{a}_i^{\\mathrm{T}}\\mathbf{a}_i\\right)^{-1}\\mathbf{a}^{\\mathrm{T}}_i\\mathbf{b}.$$\n",
    "\n",
    "By doing this with all the columns of $\\mathbf{A}$, and putting the\n",
    "results in matrix notation, we can claim that the projected version of\n",
    "$\\mathbf{b}$ *should* be equal to $\\mathbf{A}\\mathbf{x}$,\n",
    "\n",
    "$$\\ \\mathbf{A}\\mathbf{x}= \\mathbf{A}\\left(\\mathbf{A}^{\\mathrm{T}}\\mathbf{A}\\right)^{-1}\\mathbf{A}^{\\mathrm{T}}\\mathbf{b},$$\n",
    "\n",
    "$$\\ \\implies \\mathbf{x}= \\left(\\mathbf{A}^{\\mathrm{T}}\\mathbf{A}\\right)^{-1}\\mathbf{A}^{\\mathrm{T}}\\mathbf{b}$$\n",
    "\n",
    "which is the well known \\\"least squares\\\" solution. This also assumes\n",
    "$\\mathbf{A}^{\\mathrm{T}}\\mathbf{A}$ is invertible, but this issue will\n",
    "be discussed later. The \\\"least squares\\\" terminology comes from an\n",
    "alternative way of viewing the problem through calculus. Instead of\n",
    "using projections, we minimize the objective function,\n",
    "$\\mathcal{L}(\\mathbf{x})$,\n",
    "\n",
    "$$\\ \\mathcal{L}(\\mathbf{x}):=||\\mathbf{A}\\mathbf{x}-\\mathbf{b}||^2,$$\n",
    "\n",
    "which is the squared norm of the residual (error) vector . If\n",
    "$\\mathcal{L}=0$, then we can recover an exact solution to the problem\n",
    "since $\\mathbf{0}$ is the only vector with magnitude 0 and we recover\n",
    "(1). Expanding $\\mathcal{L}$, we find\n",
    "($\\left\\langle\\cdot,\\cdot\\right\\rangle$ denotes the inner product),\n",
    "\n",
    "$$\\ \\mathcal{L}=\\left\\langle\\mathbf{A}\\mathbf{x}-\\mathbf{b},\\mathbf{A}\\mathbf{x}-\\mathbf{b}\\right\\rangle$$\n",
    "\n",
    "$$\\ =\\left\\langle\\mathbf{A}\\mathbf{x},\\mathbf{A}\\mathbf{x}\\right\\rangle-2\\left\\langle\\mathbf{A}\\mathbf{x},\\mathbf{b}\\right\\rangle+\\left\\langle\\mathbf{b},\\mathbf{b}\\right\\rangle$$\n",
    "\n",
    "$$\\ =\\sum_{i=1}^m\\sum_{j=1}^n\\sum_{k=1}^nx_jA_{ji}A_{ik}x_k-2\\sum_{i=1}^m\\sum_{j=1}^nx_jA_{ji}b_i+\\sum_{i=1}^mb_i^2.$$\n",
    "\n",
    "Taking the derivative of $\\mathcal{L}$ w.r.t. each $x_p$,\n",
    "\n",
    "$$\\ \\displaystyle \\frac{\\partial^{}\\mathcal{L}}{\\partial x_p ^{}}=\\displaystyle \\frac{\\partial^{}}{\\partial x_p ^{}}\\sum_{i=1}^m\\sum_{j=1}^n\\sum_{k=1}^nx_jA_{ji}A_{ik}x_k-2\\displaystyle \\frac{\\partial^{}}{\\partial x_p ^{}}\\sum_{i=1}^m\\sum_{j=1}^nx_jA_{ji}b_i$$\n",
    "\n",
    "$$\\ =\\sum_{i=1}^m\\sum_{j=1}^n\\sum_{k=1}^n\\displaystyle \\frac{\\partial^{}x_j}{\\partial x_p ^{}}A_{ji}A_{ik}x_k+\\sum_{i=1}^m\\sum_{j=1}^n\\sum_{k=1}^nA_{ji}x_jA_{ik}\\displaystyle \\frac{\\partial^{}x_k}{\\partial x_p ^{}}-2\\sum_{i=1}^m\\sum_{j=1}^n\\displaystyle \\frac{\\partial^{}x_j}{\\partial x_p ^{}}A_{ji}b_i.$$\n",
    "\n",
    "Notice that,\n",
    "\n",
    "$$\\ \\displaystyle \\frac{\\partial^{}x_j}{\\partial x_p ^{}}=\\begin{cases}1& j=p\\\\0& j\\neq p\\end{cases};\\qquad\\displaystyle \n",
    "\\frac{\\partial^{}x_k}{\\partial x_p ^{}}=\\begin{cases}1& k=p\\\\0& k\\neq p\\end{cases},$$\n",
    "\n",
    "$$\\ \\implies \\displaystyle \\frac{\\partial^{}\\mathcal{L}}{\\partial x_p ^{}}=\\sum_{i=1}^m\\sum_{k=1}^nA_{pi}A_{ik}x_k+\\sum_{i=1}^m\\sum_{j=1}^n x_j A_{ji}A_{ip}-2\\sum_{i=1}^mA_{pi}b_i$$\n",
    "\n",
    "$$\\ =2\\sum_{i=1}^m\\sum_{k=1}^nA_{pi}A_{ik}x_k-2\\sum_{i=1}^mA_{pi}b_i$$\n",
    "\n",
    "Setting the zero derivative constraint for a stationary point, we get\n",
    "\n",
    "$$\\ 0=\\sum_{i=1}^m\\sum_{k=1}^nA_{pi}A_{ik}x_k-\\sum_{i=1}^mA_{pi}b_i$$\n",
    "\n",
    "$$\\ =\\mathbf{A}^{\\mathrm{T}}\\mathbf{A}\\mathbf{x}-\\mathbf{A}^{\\mathrm{T}}\\mathbf{b},$$\n",
    "\n",
    "and thus,\n",
    "\n",
    "$$\\ \\mathbf{A}^{\\mathrm{T}}\\mathbf{A}\\mathbf{x}=\\mathbf{A}^{\\mathrm{T}}\\mathbf{b}.$$\n",
    "\n",
    "To prove the solution is a minimum, take,\n",
    "\n",
    "$$\\ \\frac{\\partial^2\\mathcal{L}}{\\partial x_p\\partial x_q}=2\\sum_{i=1}^m\\sum_{k=1}^nA_{pi}A_{ik}\\displaystyle \\frac{\\partial^{}x_k}{\\partial x_q ^{}}$$\n",
    "\n",
    "$$\\ =2\\sum_{i=1}^nA_{pi}A_{iq}$$\n",
    "\n",
    "$$\\ =2\\mathbf{A}^{\\mathrm{T}}\\mathbf{A}$$ \n",
    "\n",
    "The above derivative is the\n",
    "\\\"Hessian\\\" matrix, for a solution to be a minimum, the Hessian must be\n",
    "positive definite. $\\mathbf{A}^{\\mathrm{T}}\\mathbf{A}$ is positive\n",
    "semi-definite, but for $\\mathbf{A}^{\\mathrm{T}}\\mathbf{A}$ to be\n",
    "invertible, it must be positive definite, so if a $\\mathbf{x}$ solution\n",
    "exists, it must be a minimum. However, if\n",
    "$\\mathbf{A}^{\\mathrm{T}}\\mathbf{A}$ is not invertible or badly\n",
    "conditioned, we cannot recover a solution or we get a very large\n",
    "$\\mathbf{x}$ due to conditioning. One way to help with this problem is\n",
    "to add a \\\"regularization\\\" term to our problem. That is, we penalize\n",
    "large $\\mathbf{x}$'s by redefining our objective function,\n",
    "\n",
    "$$\\mathcal{L}(\\mathbf{x}):=||\\mathbf{A}\\mathbf{x}-\\mathbf{b}||^2+\\lambda||\\mathbf{x}||^2,$$\n",
    "\n",
    "where $\\lambda$ is a \\\"Lagrange multiplier\\\" or more heuristically, a\n",
    "(positive) constant which determines how much we penalize large\n",
    "solutions. Yet another alternative method to solve these kinds of\n",
    "optimization problems is to constrain that the directional derivative of\n",
    "$\\mathcal{L}$ in any direction,\n",
    "$\\{\\boldsymbol{\\theta}\\in\\mathbb{R}^n ; \\ ||\\boldsymbol{\\theta}||$=1},\n",
    "is zero,\n",
    "\n",
    "$$\\ \\frac{\\partial\\mathcal{L}}{\\partial\\boldsymbol{\\theta}}=\\lim_{\\varepsilon\\to0}\\frac{\\mathcal{L}(\\mathbf{x}+\\varepsilon\\boldsymbol{\\theta})-\\mathcal{L}(\\mathbf{x})}{\\varepsilon}=0.$$\n",
    "\n",
    "Expanding the argument of the limit,\n",
    "\n",
    "$$\\ \\frac{\\mathcal{L}(\\mathbf{x}+\\varepsilon\\boldsymbol{\\theta})-\\mathcal{L}(\\mathbf{x})}{\\varepsilon}$$\n",
    "\n",
    "$$\\ =\\frac{ \\left\\langle\\mathbf{A}\\left(\\mathbf{x}+\\varepsilon\\boldsymbol{\\theta}\\right)-\\mathbf{b},\\mathbf{A}\\left(\\mathbf{x}+\\varepsilon\\boldsymbol{\\theta}\\right)-\\mathbf{b}\\right\\rangle+\\lambda\\left\\langle\\mathbf{x}+\\varepsilon\\boldsymbol{\\theta},\\mathbf{x}+\\varepsilon\\boldsymbol{\\theta}\\right\\rangle-\\left\\langle\\mathbf{A}\\mathbf{x}-\\mathbf{b},\\mathbf{A}\\mathbf{x}-\\mathbf{b}\\right\\rangle-\\lambda\\left\\langle\\mathbf{x},\\mathbf{x}\\right\\rangle}{\\varepsilon}$$\n",
    "\n",
    "$$\\ =\\frac{ \\left\\langle\\mathbf{A}\\mathbf{x},\\mathbf{A}\\mathbf{x}\\right\\rangle+2\\left\\langle\\mathbf{A}\\mathbf{x},\\mathbf{A}\\varepsilon\\boldsymbol{\\theta}\\right\\rangle+\\left\\langle\\mathbf{A}\\varepsilon\\boldsymbol{\\theta},\\mathbf{A}\\varepsilon\\boldsymbol{\\theta}\\right\\rangle-2\\left\\langle\\mathbf{b},\\mathbf{A}\\varepsilon\\boldsymbol{\\theta}\\right\\rangle-2\\left\\langle\\mathbf{A}\\mathbf{x},\\mathbf{b}\\right\\rangle+\\left\\langle\\mathbf{b},\\mathbf{b}\\right\\rangle}{\\varepsilon}$$\n",
    "\n",
    "$$\\ +\\ \\frac{\\lambda\\left\\langle\\mathbf{x},\\mathbf{x}\\right\\rangle+2\\lambda\\left\\langle\\mathbf{x},\\varepsilon\\boldsymbol{\\theta}\\right\\rangle+\\lambda\\left\\langle\\varepsilon\\boldsymbol{\\theta},\\varepsilon\\boldsymbol{\\theta}\\right\\rangle-\\left\\langle\\mathbf{A}\\mathbf{x},\\mathbf{A}\\mathbf{x}\\right\\rangle+2\\left\\langle\\mathbf{A}\\mathbf{x},\\mathbf{b}\\right\\rangle-\\left\\langle\\mathbf{b},\\mathbf{b}\\right\\rangle-\\lambda\\left\\langle\\mathbf{x},\\mathbf{x}\\right\\rangle}{\\varepsilon}$$\n",
    "\n",
    "$$\\ =2\\left\\langle\\mathbf{A}\\mathbf{x},\\mathbf{A}\\boldsymbol{\\theta}\\right\\rangle-2\\left\\langle\\mathbf{b},\\mathbf{A}\\boldsymbol{\\theta}\\right\\rangle+2\\lambda\\left\\langle\\mathbf{x},\\boldsymbol{\\theta}\\right\\rangle+\\varepsilon\\left\\langle\\mathbf{A}\\boldsymbol{\\theta},\\mathbf{A}\\boldsymbol{\\theta}\\right\\rangle+\\lambda\\varepsilon\\left\\langle\\boldsymbol{\\theta},\\boldsymbol{\\theta}\\right\\rangle.$$\n",
    "\n",
    "Taking the limit as $\\varepsilon\\to0$, we get,\n",
    "\n",
    "$$\\ \\frac{\\partial\\mathcal{L}}{\\partial\\boldsymbol{\\theta}}=2\\left\\langle\\mathbf{A}\\mathbf{x},\\mathbf{A}\\boldsymbol{\\theta}\\right\\rangle-2\\left\\langle\\mathbf{b},\\mathbf{A}\\boldsymbol{\\theta}\\right\\rangle+2\\lambda\\left\\langle\\mathbf{x},\\boldsymbol{\\theta}\\right\\rangle$$\n",
    "\n",
    "$$\\ =2\\left\\langle\\mathbf{A}^{\\mathrm{T}}\\mathbf{A}\\mathbf{x},\\boldsymbol{\\theta}\\right\\rangle+2\\lambda\\left\\langle\\mathbf{x},\\boldsymbol{\\theta}\\right\\rangle-2\\left\\langle\\mathbf{A}^{\\mathrm{T}}\\mathbf{b},\\boldsymbol{\\theta}\\right\\rangle$$\n",
    "\n",
    "$$\\ =2\\left\\langle(\\mathbf{A}^{\\mathrm{T}}\\mathbf{A}+\\lambda\\mathbf{I})\\mathbf{x}-\\mathbf{A}^{\\mathrm{T}}\\mathbf{b},\\boldsymbol{\\theta}\\right\\rangle.$$\n",
    "\n",
    "Using the zero derivative constraint,\n",
    "\n",
    "$$\\ \\left\\langle(\\mathbf{A}^{\\mathrm{T}}\\mathbf{A}+\\lambda\\mathbf{I})\\mathbf{x}-\\mathbf{A}^{\\mathrm{T}}\\mathbf{b},\\boldsymbol{\\theta}\\right\\rangle=0$$\n",
    "\n",
    "$$\\ \\implies (\\mathbf{A}^{\\mathrm{T}}\\mathbf{A}+\\lambda\\mathbf{I})\\mathbf{x}-\\mathbf{A}^{\\mathrm{T}}\\mathbf{b}=\\mathbf{0},$$\n",
    "\n",
    "or,\n",
    "\n",
    "$$\\ (\\mathbf{A}^{\\mathrm{T}}\\mathbf{A}+\\lambda\\mathbf{I})\\mathbf{x}=\\mathbf{A}^{\\mathrm{T}}\\mathbf{b}.$$\n",
    "\n",
    "This is the \\\"Ridge Regression\\\" or Tikhonov regularization problem,\n",
    "whose solution is,\n",
    "\n",
    "$$\\ \\mathbf{x}=(\\mathbf{A}^{\\mathrm{T}}\\mathbf{A}+\\lambda\\mathbf{I})^{-1}\\mathbf{A}^{\\mathrm{T}}\\mathbf{b}.$$\n",
    "\n",
    "Where $\\mathbf{A}^{\\mathrm{T}}\\mathbf{A}+\\lambda\\mathbf{I}$ can always\n",
    "be inverted due to the addition of the full rank $\\lambda\\mathbf{I}$\n",
    "matrix. To prove the solution is a minimum, we find\n",
    "\n",
    "$$\\ \\frac{\\partial^2\\mathcal{L}}{\\partial\\boldsymbol{\\theta}^2}=2\\frac{\\partial}{\\partial\\boldsymbol{\\theta}}\\left\\langle(\\mathbf{A}^{\\mathrm{T}}\\mathbf{A}+\\lambda\\mathbf{I})\\mathbf{x}-\\mathbf{A}^{\\mathrm{T}}\\mathbf{b},\\boldsymbol{\\theta}\\right\\rangle$$\n",
    "\n",
    "$$\\ =\\lim_{\\varepsilon\\to0}\\ 2\\left\\langle\\frac{(\\mathbf{A}^{\\mathrm{T}}\\mathbf{A}+\\lambda\\mathbf{I})\\mathbf{x}+(\\mathbf{A}^{\\mathrm{T}}\\mathbf{A}+\\lambda\\mathbf{I})\\varepsilon\\boldsymbol{\\theta}-\\mathbf{A}^{\\mathrm{T}}\\mathbf{b}-(\\mathbf{A}^{\\mathrm{T}}\\mathbf{A}+\\lambda\\mathbf{I})\\mathbf{x}+\\mathbf{A}^{\\mathrm{T}}\\mathbf{b}} {\\varepsilon} ,\\boldsymbol{\\theta}\\right\\rangle$$\n",
    "\n",
    "$$\\ =2\\left\\langle(\\mathbf{A}^{\\mathrm{T}}\\mathbf{A}+\\lambda\\mathbf{I})\\boldsymbol{\\theta},\\boldsymbol{\\theta}\\right\\rangle$$\n",
    "\n",
    "$$\\ =2\\left\\langle\\mathbf{A}^{\\mathrm{T}}\\mathbf{A}\\boldsymbol{\\theta},\\boldsymbol{\\theta}\\right\\rangle+2\\lambda\\left\\langle\\boldsymbol{\\theta},\\boldsymbol{\\theta}\\right\\rangle$$\n",
    "\n",
    "$$\\ =2\\left\\langle\\mathbf{A}\\boldsymbol{\\theta},\\mathbf{A}\\boldsymbol{\\theta}\\right\\rangle+2\\lambda\\left\\langle\\boldsymbol{\\theta},\\boldsymbol{\\theta}\\right\\rangle$$\n",
    "\n",
    "$$\\ =2||\\mathbf{A}\\boldsymbol{\\theta}||^2+2\\lambda||\\boldsymbol{\\theta}||^2>0.$$\n",
    "\n",
    "Thus, the solution to the regularized least squares problem is a minimal\n",
    "norm solution. Yet another way to interpret this is as a variational\n",
    "problem where $\\boldsymbol{\\theta}$ is some variation of $\\mathbf{x}$.\n",
    "If $\\mathbf{x}$ is a minimizer of $\\mathcal{L}$, then it is necessary\n",
    "that any variation of $\\mathbf{x}$ be $\\mathbf{0}$. This results in the same\n",
    "calculations as above, but the interpretations of the involved\n",
    "calculations are subtly different."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
